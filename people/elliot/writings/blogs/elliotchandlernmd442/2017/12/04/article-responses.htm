<h1>Article responses: 2017 Dec. 04</h1>

<h2>Response to "<a href="https://www.wired.com/2015/12/simplicity-is-overrated-in-ux-design/">When It Comes to UX Design, Simplicity Is Overrated</a>"</h2>
<p><em>Article by Robert Hoekman Jr.: 18 December 2015, in </em>Wired</p>
<p></p>

<h2>Response to "<a href="https://medium.com/@mikeatherton/ux-is-ui-105460807734">UX is UI</a>"</h2>
<p><em>Article by Mike Atherton: 15 May 2015, in </em>Medium</p>
<p>Response</p>

<h2>Response to "<a href="https://www.fastcodesign.com/3048139/what-is-zero-ui-and-why-is-it-crucial-to-the-future-of-design">When It Comes to UX Design, Simplicity Is Overrated</a>"</h2>
<p><em>Article by John Brownlee: 2 July 2015, in </em>Co.Design</p>
<p>Response</p>

<h2>Response to "<a href="https://blog.prototypr.io/good-design-doesnt-make-people-happy-a90e2f033ba0">Good design doesn't make people happy.</a>"</h2>
<p><em>Article by John Warren Hanawalt: 27 November 2017, in </em>Prototypr</p>
<p>Response</p>

<h2>Response to "<a href="https://uxmag.com/creating-a-chatbot">Creating a Chatbot: A UX Designer’s Firsthand Experience</a>"</h2>
<p><em>Article by Scott Milburn: 7 November 2017, in </em>Wired</p>
<p>This article seems overly optimistic to me. While artificial intelligence is certainly progressing rapidly, in my experience chatbots, at least the simple ones that are commonly available, are nowhere near capable of having a meaningful, effective conversation. Considering that Google's and Apple's conversational interfaces are quite prone to misinterpreting queries, and they only attempt to provide basic information with brief context (such as based on other recent queries), I think that a chatbot that would be able to actually provide sufficiently nuanced responses to be conversational is quite a ways in the future. The idea of a bot to respond to a nagging parent proposed in the article would need such nuance to be useful, and would need a contextual understanding of the person whose role it is filling and of that person's life. The sort of responses shown in the article, I think, would be more likely to have the parent wondering what sort of drugs their kid is on than to reassure them, simply because of the repetitiveness and frequent nonsequiturs. The article suggests that only further data training is necessary, but I disagree — a system like this falls apart as soon as it is asked to reason about something outside of its experience, or with more complexity software traditionally does. For instance, if the parent asked, "What would you like for supper tonight?" and the person's bot replied "Greens.", and then the parent wrote back saying "Well, Sally will be over, if you catch my drift?"</p>
